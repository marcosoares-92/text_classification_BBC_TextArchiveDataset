{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "colab": {
            "collapsed_sections": [],
            "name": "C3_W2_Lab_3_imdb_subwords.ipynb",
            "private_outputs": true,
            "provenance": []
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C3/W2/ungraded_labs/C3_W2_Lab_3_imdb_subwords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ],
            "metadata": {
                "azdata_cell_guid": "f3c77183-2fa3-48f9-907d-ff6332234eca"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Ungraded Lab: Subword Tokenization with the IMDB Reviews Dataset\n",
                "\n",
                "In this lab, you will look at a pre-tokenized dataset that is using subword text encoding. This is an alternative to word-based tokenization which you have been using in the previous labs. You will see how it works and its implications on preparing your data and training your model.\n",
                "\n",
                "Let's begin!\n"
            ],
            "metadata": {
                "id": "cLKIel77CJPi",
                "azdata_cell_guid": "67d0103a-af4d-4f3e-806b-33ad56c646da"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Download the IMDB reviews plain text and tokenized datasets\n",
                "\n",
                "First, you will download the [IMDB Reviews](https://www.tensorflow.org/datasets/catalog/imdb_reviews) dataset from Tensorflow Datasets. You will get two configurations:\n",
                "\n",
                "* `plain_text` - this is the default and the one you used in Lab 1 of this week\n",
                "* `subwords8k` - a pre-tokenized dataset (i.e. instead of sentences of type string, it will already give you the tokenized sequences). You will see how this looks in later sections."
            ],
            "metadata": {
                "id": "qrzOn9quZ0Sv",
                "azdata_cell_guid": "c3cd8870-ec7c-4fa5-8e38-bb85315105ab"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import tensorflow_datasets as tfds\n",
                "\n",
                "# Download the plain text default config\n",
                "imdb_plaintext, info_plaintext = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
                "\n",
                "# Download the subword encoded pretokenized dataset\n",
                "imdb_subwords, info_subwords = tfds.load(\"imdb_reviews/subwords8k\", with_info=True, as_supervised=True)"
            ],
            "metadata": {
                "id": "_IoM4VFxWpMR",
                "azdata_cell_guid": "397f6715-4268-426e-b8de-7136ceffff2b",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Compare the two datasets\n",
                "\n",
                "As mentioned, the data types returned by the two datasets will be different. For the default, it will be strings as you also saw in Lab 1. Notice the description of the `text` key below and the sample sentences:"
            ],
            "metadata": {
                "id": "JggMZRCEcdlN",
                "azdata_cell_guid": "962bcd5d-63c9-4383-9b59-319f779d6b91"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Print description of features\n",
                "info_plaintext.features"
            ],
            "metadata": {
                "id": "3J7IAJMGH-VN",
                "azdata_cell_guid": "ed0cc488-c330-4dea-bb19-3ffdd936424d",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Take 2 training examples and print the text feature\n",
                "for example in imdb_plaintext['train'].take(2):\n",
                "  print(example[0].numpy())"
            ],
            "metadata": {
                "id": "jTO45ghxc4js",
                "azdata_cell_guid": "37683afe-d18e-44e9-96aa-0b3ff065cbc8",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "For `subwords8k`, the dataset is already tokenized so the data type will be integers. Notice that the `text` features also include an `encoder` field and has a `vocab_size` of around 8k, hence the name."
            ],
            "metadata": {
                "id": "f87JvGD9dId5",
                "azdata_cell_guid": "04a050fc-6bc0-42e6-9512-fe89497ceda2"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Print description of features\n",
                "info_subwords.features"
            ],
            "metadata": {
                "id": "3wp_a7292mxk",
                "azdata_cell_guid": "4a7465db-cf09-425c-87cb-cb3a2a326e0a",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "If you print the results, you will not see string sentences but a sequence of tokens:"
            ],
            "metadata": {
                "id": "9ssDU_TddyLF",
                "azdata_cell_guid": "945e0695-0a28-436b-9abd-840e20611e9e"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Take 2 training examples and print its contents\n",
                "for example in imdb_subwords['train'].take(2):\n",
                "  print(example)"
            ],
            "metadata": {
                "id": "35oQQIUG21cG",
                "azdata_cell_guid": "03de44cc-066a-4abe-8c7e-5a58c22bf645",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "You can get the `encoder` object included in the download and use it to decode the sequences above. You'll see that you will arrive at the same sentences provided in the `plain_text` config:"
            ],
            "metadata": {
                "id": "rWOrkYGug--B",
                "azdata_cell_guid": "7d82b8fe-e39a-4779-9578-1b3e44152f68"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Get the encoder\n",
                "tokenizer_subwords = info_subwords.features['text'].encoder\n",
                "\n",
                "# Take 2 training examples and decode the text feature\n",
                "for example in imdb_subwords['train'].take(2):\n",
                "  print(tokenizer_subwords.decode(example[0]))"
            ],
            "metadata": {
                "id": "4kNEGgEgfO6x",
                "azdata_cell_guid": "83a1a8a0-2eb1-4467-92f0-d3a808acb268",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "*Note: The documentation for the encoder can be found [here](https://www.tensorflow.org/datasets/api_docs/python/tfds/deprecated/text/SubwordTextEncoder) but don't worry if it's marked as deprecated. As mentioned, the objective of this exercise is just to show the characteristics of subword encoding.*"
            ],
            "metadata": {
                "id": "20_XNWbXiwcE",
                "azdata_cell_guid": "e7927f17-31b0-4be7-b68e-0fc2a66c53ab"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Subword Text Encoding\n",
                "\n",
                "From previous labs, the number of tokens in the sequence is the same as the number of words in the text (i.e. word tokenization). The following cells shows a review of this process."
            ],
            "metadata": {
                "id": "YKrbY2fjjFHM",
                "azdata_cell_guid": "3286a3e6-353f-4817-a0a3-f1be857ff4d4"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Get the train set\n",
                "train_data = imdb_plaintext['train']\n",
                "\n",
                "# Initialize sentences list\n",
                "training_sentences = []\n",
                "\n",
                "# Loop over all training examples and save to the list\n",
                "for s,_ in train_data:\n",
                "  training_sentences.append(s.numpy().decode('utf8'))"
            ],
            "metadata": {
                "id": "O6ly_yOIkM-K",
                "azdata_cell_guid": "95fd07bf-4606-46b8-8666-5e905da22df1",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "from tensorflow.keras.preprocessing.text import Tokenizer\n",
                "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
                "\n",
                "vocab_size = 10000\n",
                "oov_tok = '<OOV>'\n",
                "\n",
                "# Initialize the Tokenizer class\n",
                "tokenizer_plaintext = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
                "\n",
                "# Generate the word index dictionary for the training sentences\n",
                "tokenizer_plaintext.fit_on_texts(training_sentences)\n",
                "\n",
                "# Generate the training sequences\n",
                "sequences = tokenizer_plaintext.texts_to_sequences(training_sentences)"
            ],
            "metadata": {
                "id": "-N6Yd_TE3gZ5",
                "azdata_cell_guid": "e8639288-2344-45f8-8d9d-1a5dfa155933",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "The cell above uses a `vocab_size` of 10000 but you'll find that it's easy to find OOV tokens when decoding using the lookup dictionary it created. See the result below:"
            ],
            "metadata": {
                "id": "nNUlDp76lf94",
                "azdata_cell_guid": "f21972b8-dac8-4493-946a-e52dd8fd7773"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Decode the first sequence using the Tokenizer class\n",
                "tokenizer_plaintext.sequences_to_texts(sequences[0:1])"
            ],
            "metadata": {
                "id": "YmsECyVr4OPE",
                "azdata_cell_guid": "6882446f-4647-4205-a4ee-8a3d199505cf",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "For binary classifiers, this might not have a big impact but you may have other applications that will benefit from avoiding OOV tokens when training the model (e.g. text generation). If you want the tokenizer above to not have OOVs, then the `vocab_size` will increase to more than 88k. This can slow down training and bloat the model size. The encoder also won't be robust when used on other datasets which may contain new words, thus resulting in OOVs again. "
            ],
            "metadata": {
                "id": "O0HQqkBmpujb",
                "azdata_cell_guid": "90a1c4ba-93d4-4f26-9402-6e1956cc59bb"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Total number of words in the word index dictionary\n",
                "len(tokenizer_plaintext.word_index)"
            ],
            "metadata": {
                "id": "u7m-Ds9lpUQc",
                "azdata_cell_guid": "918b58bb-45e3-49d6-9d87-ca4617617ee5",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "*Subword text encoding* gets around this problem by using parts of the word to compose whole words. This makes it more flexible when it encounters uncommon words. See how these subwords look like for this particular encoder:"
            ],
            "metadata": {
                "id": "McxNKhHIsNvl",
                "azdata_cell_guid": "b8561a48-d613-42f3-bd28-8b7dae657723"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Print the subwords\n",
                "print(tokenizer_subwords.subwords)"
            ],
            "metadata": {
                "id": "SqyMSZbnwFBo",
                "azdata_cell_guid": "0714eef0-acbf-498f-876b-fda8677fe9e1",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "If you use it on the previous plain text sentence, you'll see that it won't have any OOVs even if it has a smaller vocab size (only 8k compared to 10k above):"
            ],
            "metadata": {
                "id": "kaRA9LBUwfHM",
                "azdata_cell_guid": "ca68b20c-54b7-4a53-9298-5a8845dd593e"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Encode the first plaintext sentence using the subword text encoder\n",
                "tokenized_string = tokenizer_subwords.encode(training_sentences[0])\n",
                "print(tokenized_string)\n",
                "\n",
                "# Decode the sequence\n",
                "original_string = tokenizer_subwords.decode(tokenized_string)\n",
                "\n",
                "# Print the result\n",
                "print (original_string)"
            ],
            "metadata": {
                "id": "tn_eLaS5mR7H",
                "azdata_cell_guid": "efdc18f3-c74c-4f90-b930-67ae04f72228",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Subword encoding can even perform well on words that are not commonly found on movie reviews. See first the result when using the plain text tokenizer. As expected, it will show many OOVs:"
            ],
            "metadata": {
                "id": "iL9O3hEqw4Bl",
                "azdata_cell_guid": "28dbb562-d4d2-4d6b-95d2-b1b071fc9fd7"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Define sample sentence\n",
                "sample_string = 'TensorFlow, from basics to mastery'\n",
                "\n",
                "# Encode using the plain text tokenizer\n",
                "tokenized_string = tokenizer_plaintext.texts_to_sequences([sample_string])\n",
                "print ('Tokenized string is {}'.format(tokenized_string))\n",
                "\n",
                "# Decode and print the result\n",
                "original_string = tokenizer_plaintext.sequences_to_texts(tokenized_string)\n",
                "print ('The original string: {}'.format(original_string))"
            ],
            "metadata": {
                "id": "MHRj1J0j8ApE",
                "azdata_cell_guid": "e885103d-3ec4-4e7d-9b4d-b98a7d492062",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Then compare to the subword text encoder:"
            ],
            "metadata": {
                "id": "ZhQ-4O-uxdbJ",
                "azdata_cell_guid": "0176fefc-4687-4278-9b3b-d283ac495ef4"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Encode using the subword text encoder\n",
                "tokenized_string = tokenizer_subwords.encode(sample_string)\n",
                "print ('Tokenized string is {}'.format(tokenized_string))\n",
                "\n",
                "# Decode and print the results\n",
                "original_string = tokenizer_subwords.decode(tokenized_string)\n",
                "print ('The original string: {}'.format(original_string))\n"
            ],
            "metadata": {
                "id": "fPl2BXhYEHRP",
                "azdata_cell_guid": "85cb3b68-841f-4886-8eb7-60c86bb717b8",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "As you may notice, the sentence is correctly decoded. The downside is the token sequence is much longer. Instead of only 5 when using word-encoding, you ended up with 11 tokens instead. The mapping for this sentence is shown below:"
            ],
            "metadata": {
                "id": "89sbfXjz0MSW",
                "azdata_cell_guid": "448e2f8f-0276-4af0-82a1-cc14e49dca15"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Show token to subword mapping:\n",
                "for ts in tokenized_string:\n",
                "  print ('{} ----> {}'.format(ts, tokenizer_subwords.decode([ts])))"
            ],
            "metadata": {
                "id": "_3t7vvNLEZml",
                "azdata_cell_guid": "ec883e07-c4a7-402b-9016-a62ab037e3d8",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Training the model\n",
                "\n",
                "You will now train your model using this pre-tokenized dataset. Since these are already saved as sequences, you can jump straight to making uniform sized arrays for the train and test sets. These are also saved as `tf.data.Dataset` type so you can use the [`padded_batch()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch) method to create batches and pad the arrays into a uniform size for training."
            ],
            "metadata": {
                "id": "aZ22ugch1TFy",
                "azdata_cell_guid": "193d94f8-e951-4000-829b-5835ed64ca81"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "BUFFER_SIZE = 10000\n",
                "BATCH_SIZE = 64\n",
                "\n",
                "# Get the train and test splits\n",
                "train_data, test_data = imdb_subwords['train'], imdb_subwords['test'], \n",
                "\n",
                "# Shuffle the training data\n",
                "train_dataset = train_data.shuffle(BUFFER_SIZE)\n",
                "\n",
                "# Batch and pad the datasets to the maximum length of the sequences\n",
                "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\n",
                "test_dataset = test_data.padded_batch(BATCH_SIZE)"
            ],
            "metadata": {
                "id": "LVSTLBe_SOUr",
                "azdata_cell_guid": "a82d6964-11a6-4d5e-8eee-560487278700",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Next, you will build the model. You can just use the architecture from the previous lab. "
            ],
            "metadata": {
                "id": "HCjHCG7s2sAR",
                "azdata_cell_guid": "b083cfc4-b29e-471f-bcdb-68754eb3f69d"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import tensorflow as tf\n",
                "\n",
                "# Define dimensionality of the embedding\n",
                "# Number of dimensions of the embedding vectors - conversion of words into vectors in an\n",
                "# N-dimensional space that minimizes the distance between vectors correspondent to words from the\n",
                "# same semantical field (i.e., similar meaning or feeling). Each vector has embedding_dim dimensions,\n",
                "# where each dimension of the vector is occupied by a float value.\n",
                "embedding_dim = 64\n",
                "\n",
                "# Build the model\n",
                "model = tf.keras.Sequential([\n",
                "    tf.keras.layers.Embedding(tokenizer_subwords.vocab_size, embedding_dim),\n",
                "    tf.keras.layers.GlobalAveragePooling1D(),\n",
                "    tf.keras.layers.Dense(6, activation='relu'),\n",
                "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
                "])\n",
                "\n",
                "# Print the model summary\n",
                "model.summary()"
            ],
            "metadata": {
                "id": "5NEpdhb8AxID",
                "azdata_cell_guid": "8eaa65a2-8a97-413f-a80f-66c5c9b61a23",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Similarly, you can use the same parameters for training. In Colab, it will take around 20 seconds per epoch (without an accelerator) and you will reach around 94% training accuracy and 88% validation accuracy."
            ],
            "metadata": {
                "id": "2aOn2bAc3AUj",
                "azdata_cell_guid": "a19cbd6b-b5f8-45ea-80d5-0e57aa8027c7"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "num_epochs = 10\n",
                "\n",
                "# Set the training parameters\n",
                "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
                "\n",
                "# Start training\n",
                "history = model.fit(train_dataset, epochs=num_epochs, validation_data=test_dataset)"
            ],
            "metadata": {
                "id": "fkt8c5dNuUlT",
                "azdata_cell_guid": "7c020db2-3a92-4ce3-8ca4-06f14c74b1ed",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Visualize the results\n",
                "\n",
                "You can use the cell below to plot the training results. See if you can improve it by tweaking the parameters such as the size of the embedding and number of epochs."
            ],
            "metadata": {
                "id": "3ygYaD6H3qGX",
                "azdata_cell_guid": "7e426294-09cc-4fc2-adfb-d080b2388d9c"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Plot utility\n",
                "def plot_graphs(history, string):\n",
                "  plt.plot(history.history[string])\n",
                "  plt.plot(history.history['val_'+string])\n",
                "  plt.xlabel(\"Epochs\")\n",
                "  plt.ylabel(string)\n",
                "  plt.legend([string, 'val_'+string])\n",
                "  plt.show()\n",
                "\n",
                "# Plot the accuracy and results \n",
                "plot_graphs(history, \"accuracy\")\n",
                "plot_graphs(history, \"loss\")"
            ],
            "metadata": {
                "id": "-_rMnm7WxQGT",
                "azdata_cell_guid": "ca97d334-648d-40b1-b646-39db24d4042a",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Wrap Up\n",
                "\n",
                "In this lab, you saw how subword text encoding can be a robust technique to avoid out-of-vocabulary tokens. It can decode uncommon words it hasn't seen before even with a relatively small vocab size. Consequently, it results in longer token sequences when compared to full word tokenization. Next week, you will look at other architectures that you can use when building your classifier. These will be recurrent neural networks and convolutional neural networks."
            ],
            "metadata": {
                "id": "R0TRE-Lb4C5b",
                "azdata_cell_guid": "982977ca-71ac-46b7-b75e-c336e1604448"
            }
        }
    ]
}